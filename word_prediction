import json
import os
import numpy as np
import tensorflow as tf
import src.model as model
import src.sample as sample
import src.encoder as encoder
import dash
import dash_core_components as dcc
import dash_html_components as html
from dash.dependencies import Input, Output

app = dash.Dash()
total_predicted_word = []
text_output = []

app.layout = html.Div([
        html.H3("Fake news generator and  % of fields it generates fake news",
                style={'display':'center', 'verticalAlign':'bottom', 'width':'70%','textAlign': 'center', 'margin':25}),

    html.Div([
    dcc.Input(id='my-id', value='What is your', type='text')],
        style={'display':'inline-block'}),

    html.Div([
        html.Button(
            id='submit-button',
            n_clicks=0,
            children='Submit',
            style={'fontSize':24, 'marginLeft':'30px'}
        ),
    ], style={'display':'inline-block'}),
    html.Div(id='my-div')
])

def interact_model(
    model_name,
    seed,
    nsamples,
    batch_size,
    length,
    temperature,
    top_k,
    models_dir,
        text_raw
):
    print(nsamples)
    # global total_predicted_word
    total_predicted_word = []
    models_dir = os.path.expanduser(os.path.expandvars(models_dir))
    if batch_size is None:
        batch_size = 1
    assert nsamples % batch_size == 0

    enc = encoder.get_encoder(model_name, models_dir)
    hparams = model.default_hparams()
    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:
        hparams.override_from_dict(json.load(f))

    if length is None:
        length = hparams.n_ctx // 2
    elif length > hparams.n_ctx:
        raise ValueError("Can't get samples longer than window size: %s" % hparams.n_ctx)

    with tf.Session(graph=tf.Graph()) as sess:
        context = tf.placeholder(tf.int32, [batch_size, None])
        np.random.seed(seed)
        tf.set_random_seed(seed)
        output = sample.sample_sequence(
            hparams=hparams, length=length,
            context=context,
            batch_size=batch_size,
            temperature=temperature, top_k=top_k
        )

        saver = tf.train.Saver()
        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))
        saver.restore(sess, ckpt)

        raw_text = text_raw
        while not raw_text:
            print('Prompt should not be empty!')
            raw_text = input("Model prompt >>> ")
        context_tokens = enc.encode(raw_text)
        generated = 0
        for _ in range(nsamples // batch_size):
            out = sess.run(output, feed_dict={
                context: [context_tokens for _ in range(batch_size)]
            })[:, len(context_tokens):]
            for i in range(batch_size):
                generated += 1
                text = enc.decode(out[i])
                print("=" * 40 + " SAMPLE " + str(generated) + " " + "=" * 40)
                total_predicted_word.append(str(text))
        print(total_predicted_word)
        return total_predicted_word


@app.callback(
    Output(component_id='my-div', component_property='children'),
    [Input(component_id='my-id', component_property='value')]
)
def update_output_div(input_value):
    raw_what_text = str(input_value.strip())
    if input_value.endswith(' '):
        text_output = interact_model(model_name='345M',
                       seed=None,
                       nsamples=1,
                       batch_size=1,
                       length=400,
                       temperature=1,
                       top_k=40,
                       models_dir='/home/manmohan/projects_on_python/gpt-2/models',
                       text_raw=raw_what_text)
        return text_output
    else:
        text_output = total_predicted_word
        return text_output

if __name__ == '__main__':
    app.run_server()

